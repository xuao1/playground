Running iteration 1 with perception_scale 1.0 and generation_scale 2.0...
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:461: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Namespace(query_interval=1, input_seq_len=8, encoder_n=1, prefill_n=1, decode_n=1, encoder_len=1, prefill_len=1, decode_len=6, simu_ts_len=500, real_run=True, enable_recompute=False, mode='ours', req_interval=0.1362, model='openvla', dim=512, enable_slice=False, warmup_num=10, trail_num=20, profile_mode='flashinfer', only_profile=False, num_trails=200, worker_num=3, verbose=0, config='./config_ours.yaml', diffusion_step=100, diffusion_stage_num=5, input_img_num=1, input_img_size=96, input_traj_cnn_size=16, input_traj_transformer_size=10, input_cond_size=66, input_dim=2, cond_dim=66, n_emb=256, global_cond_dim=132, perception_scale=1.0, generation_scale=2.0)
['e', 'p', 'd', 'd', 'd', 'd', 'd', 'd']
8
---------
('e',): 1
('p', 'e'): 1
('d', 'p', 'e'): 1
('d', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'd', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'd', 'd', 'd', 'p', 'e'): 493
['e', 'p', 'd', 'd', 'd', 'd', 'd', 'd']
8
---------
(0,): 1
(1, 0): 1
(2, 1, 0): 1
(3, 2, 1, 0): 1
(4, 3, 2, 1, 0): 1
(5, 4, 3, 2, 1, 0): 1
(6, 5, 4, 3, 2, 1, 0): 1
(7, 6, 5, 4, 3, 2, 1, 0): 493
Start OpenVLA inference...
self.n_replica:  1
Perception params: 7.182803e+08
Generation params: 1.128453e+10
Traceback (most recent call last):
  File "/workspace/xformers/playground/sche_plan.py", line 317, in <module>
    profile_data = openvla_run(sche_plan = sche_plan, mode = args.mode)
  File "/workspace/xformers/playground/openvla_inference/inference.py", line 527, in openvla_run
    e = OpenVLA_engine()
  File "/workspace/xformers/playground/openvla_inference/inference.py", line 77, in __init__
    self.generate_cuda_graphs()
  File "/workspace/xformers/playground/openvla_inference/inference.py", line 88, in generate_cuda_graphs
    out, new_cache = self.models['llm'].wrapped_decoder.make_graph(self.caches['text'][0], 
  File "/workspace/xformers/playground/../../repos/x-transformers/x_transformers/autoregressive_wrapper.py", line 313, in make_graph
    logits, new_cache = self.net(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py", line 1788, in forward
    x, intermediates = self.attn_layers(x, mask = mask, mems = mems, mem_masks = mem_masks, cache = cache, return_hiddens = True, seq_start_pos = seq_start_pos, slice_num = slice_num, slice_id = slice_id, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py", line 1406, in forward
    out = block(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py", line 663, in forward
    return self.ff(x)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 1 has a total capacity of 23.64 GiB of which 22.25 MiB is free. Process 1315637 has 23.60 GiB memory in use. Of the allocated memory 22.77 GiB is allocated by PyTorch, and 368.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Iteration 1 completed.
--------------------------------------
Running iteration 2 with perception_scale 1.0 and generation_scale 2.0...
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:461: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
Namespace(query_interval=1, input_seq_len=8, encoder_n=1, prefill_n=1, decode_n=1, encoder_len=1, prefill_len=1, decode_len=6, simu_ts_len=500, real_run=True, enable_recompute=False, mode='ours', req_interval=0.1362, model='openvla', dim=512, enable_slice=False, warmup_num=10, trail_num=20, profile_mode='flashinfer', only_profile=False, num_trails=200, worker_num=3, verbose=0, config='./config_ours.yaml', diffusion_step=100, diffusion_stage_num=5, input_img_num=1, input_img_size=96, input_traj_cnn_size=16, input_traj_transformer_size=10, input_cond_size=66, input_dim=2, cond_dim=66, n_emb=256, global_cond_dim=132, perception_scale=1.0, generation_scale=2.0)
['e', 'p', 'd', 'd', 'd', 'd', 'd', 'd']
8
---------
('e',): 1
('p', 'e'): 1
('d', 'p', 'e'): 1
('d', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'd', 'd', 'p', 'e'): 1
('d', 'd', 'd', 'd', 'd', 'd', 'p', 'e'): 493
['e', 'p', 'd', 'd', 'd', 'd', 'd', 'd']
8
---------
(0,): 1
(1, 0): 1
(2, 1, 0): 1
(3, 2, 1, 0): 1
(4, 3, 2, 1, 0): 1
(5, 4, 3, 2, 1, 0): 1
(6, 5, 4, 3, 2, 1, 0): 1
(7, 6, 5, 4, 3, 2, 1, 0): 493
Traceback (most recent call last):
  File "/workspace/xformers/playground/sche_plan.py", line 316, in <module>
    from openvla_inference.inference import openvla_run
  File "/workspace/xformers/playground/openvla_inference/inference.py", line 15, in <module>
    from .encoder.dinov2 import DINOv2
  File "/workspace/xformers/playground/openvla_inference/encoder/dinov2.py", line 3, in <module>
    from beartype import beartype
  File "/usr/local/lib/python3.10/dist-packages/beartype/__init__.py", line 158, in <module>
    from beartype._decor.decormain import (
  File "/usr/local/lib/python3.10/dist-packages/beartype/_decor/decormain.py", line 112, in <module>
    from beartype._decor.decorcache import beartype
  File "/usr/local/lib/python3.10/dist-packages/beartype/_decor/decorcache.py", line 38, in <module>
    from beartype._decor.decorcore import beartype_object
  File "/usr/local/lib/python3.10/dist-packages/beartype/_decor/decorcore.py", line 26, in <module>
    from beartype._decor._decornontype import beartype_nontype
  File "/usr/local/lib/python3.10/dist-packages/beartype/_decor/_decornontype.py", line 40, in <module>
    from beartype._decor.wrap.wrapmain import generate_code
  File "/usr/local/lib/python3.10/dist-packages/beartype/_decor/wrap/wrapmain.py", line 35, in <module>
    from beartype._decor.wrap._wrapargs import (
  File "/usr/local/lib/python3.10/dist-packages/beartype/_decor/wrap/_wrapargs.py", line 33, in <module>
    from beartype._check.checkmake import make_code_raiser_func_pith_check
  File "/usr/local/lib/python3.10/dist-packages/beartype/_check/checkmake.py", line 31, in <module>
    from beartype._check.convert.convsanify import sanify_hint_root_statement
  File "/usr/local/lib/python3.10/dist-packages/beartype/_check/convert/convsanify.py", line 26, in <module>
    from beartype._check.convert.convreduce import reduce_hint
  File "/usr/local/lib/python3.10/dist-packages/beartype/_check/convert/convreduce.py", line 94, in <module>
    from beartype._util.hint.nonpep.api.utilmodnumpy import (
  File "/usr/local/lib/python3.10/dist-packages/beartype/_util/hint/nonpep/api/utilmodnumpy.py", line 32, in <module>
    from beartype._util.hint.pep.utilpepget import get_hint_pep_args
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
KeyboardInterrupt
