Testing with worker_num=8
Running iteration 1 with worker_num=8...
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:461: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
Namespace(query_interval=1, input_seq_len=8, encoder_n=1, prefill_n=1, decode_n=1, encoder_len=1, prefill_len=1, decode_len=10, simu_ts_len=500, real_run=True, enable_recompute=False, mode='parallel_v2', req_interval=0.2477, model='diffusion_cnn', dim=512, enable_slice=False, warmup_num=10, trail_num=20, profile_mode='flashinfer', only_profile=False, num_trails=200, worker_num=8, verbose=0, config='./config_parallel.yaml', diffusion_step=100, diffusion_stage_num=5)
Start DiffusionPolicy inference...
data_type:  torch.bfloat16
====== Graph for resnet generated ======
====== Graph for backbone generated ======
Query latency: 856.462 ms
Query duration: 254.989
Throughput: 3.922
DiffusionPolicy inference finished.
Iteration 1 completed.
--------------------------------------
Running iteration 2 with worker_num=8...
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:461: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
Namespace(query_interval=1, input_seq_len=8, encoder_n=1, prefill_n=1, decode_n=1, encoder_len=1, prefill_len=1, decode_len=10, simu_ts_len=500, real_run=True, enable_recompute=False, mode='parallel_v2', req_interval=0.2477, model='diffusion_cnn', dim=512, enable_slice=False, warmup_num=10, trail_num=20, profile_mode='flashinfer', only_profile=False, num_trails=200, worker_num=8, verbose=0, config='./config_parallel.yaml', diffusion_step=100, diffusion_stage_num=5)
Start DiffusionPolicy inference...
data_type:  torch.bfloat16
====== Graph for resnet generated ======
====== Graph for backbone generated ======
Query latency: 866.094 ms
Query duration: 255.069
Throughput: 3.921
DiffusionPolicy inference finished.
Iteration 2 completed.
--------------------------------------
Running iteration 3 with worker_num=8...
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:461: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
Namespace(query_interval=1, input_seq_len=8, encoder_n=1, prefill_n=1, decode_n=1, encoder_len=1, prefill_len=1, decode_len=10, simu_ts_len=500, real_run=True, enable_recompute=False, mode='parallel_v2', req_interval=0.2477, model='diffusion_cnn', dim=512, enable_slice=False, warmup_num=10, trail_num=20, profile_mode='flashinfer', only_profile=False, num_trails=200, worker_num=8, verbose=0, config='./config_parallel.yaml', diffusion_step=100, diffusion_stage_num=5)
Start DiffusionPolicy inference...
data_type:  torch.bfloat16
====== Graph for resnet generated ======
====== Graph for backbone generated ======
Query latency: 896.355 ms
Query duration: 255.436
Throughput: 3.915
DiffusionPolicy inference finished.
Iteration 3 completed.
--------------------------------------
Running iteration 4 with worker_num=8...
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:461: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
Namespace(query_interval=1, input_seq_len=8, encoder_n=1, prefill_n=1, decode_n=1, encoder_len=1, prefill_len=1, decode_len=10, simu_ts_len=500, real_run=True, enable_recompute=False, mode='parallel_v2', req_interval=0.2477, model='diffusion_cnn', dim=512, enable_slice=False, warmup_num=10, trail_num=20, profile_mode='flashinfer', only_profile=False, num_trails=200, worker_num=8, verbose=0, config='./config_parallel.yaml', diffusion_step=100, diffusion_stage_num=5)
Start DiffusionPolicy inference...
data_type:  torch.bfloat16
====== Graph for resnet generated ======
====== Graph for backbone generated ======
Query latency: 872.958 ms
Query duration: 256.417
Throughput: 3.900
DiffusionPolicy inference finished.
Iteration 4 completed.
--------------------------------------
Running iteration 5 with worker_num=8...
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
/workspace/xformers/playground/../../repos/x-transformers/x_transformers/x_transformers.py:461: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @autocast(enabled = False)
Namespace(query_interval=1, input_seq_len=8, encoder_n=1, prefill_n=1, decode_n=1, encoder_len=1, prefill_len=1, decode_len=10, simu_ts_len=500, real_run=True, enable_recompute=False, mode='parallel_v2', req_interval=0.2477, model='diffusion_cnn', dim=512, enable_slice=False, warmup_num=10, trail_num=20, profile_mode='flashinfer', only_profile=False, num_trails=200, worker_num=8, verbose=0, config='./config_parallel.yaml', diffusion_step=100, diffusion_stage_num=5)
Start DiffusionPolicy inference...
data_type:  torch.bfloat16
====== Graph for resnet generated ======
====== Graph for backbone generated ======
Query latency: 883.782 ms
Query duration: 255.538
Throughput: 3.913
DiffusionPolicy inference finished.
Iteration 5 completed.
--------------------------------------
All iterations for worker_num=8 are complete. Output saved to diffusion_cnn_parallel_worker8.log.
Average Query Duration: 255.49 ms (0.25549000 seconds)
Throughput: 3.91404751 requests/second
====================================
